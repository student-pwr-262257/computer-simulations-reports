---
date: today
date-format: long
title: Computer Simulations
subtitle: Report 1
authors:
- name: Karolina Jonczyk
  affiliations:
  - name: Wrocław University of Science and Technology
    city: Wrocław
- name: Jakub Kaczor
  email: 262257@student.pwr.edu.pl
  affiliations:
  - name: Wrocław University of Science and Technology
    city: Wrocław

callout-appearance: simple
jupyter: julia-1.9
---

# Statistics

In the following work, the authors are going to assess the implemented
methods by various means. Descriptive statistics are one of them. Here,
they present the implementation of each.

```{julia}
using Test
# Used only for testing
import Statistics as Stats
import StatsBase

const TEST_SAMPLE = randn(10^3)

"Compute arithmetic mean over the elements of an array"
function mean(A::AbstractArray)
   sum(A) / length(A)
end

@test mean(TEST_SAMPLE) ≈ Stats.mean(TEST_SAMPLE)

"Compute variance over the elements of an array"
function var(A::AbstractArray, mean=mean(A))
   sum(@. (A - mean)^2) / (length(A) - 1)
end

@test var(TEST_SAMPLE) ≈ Stats.var(TEST_SAMPLE)

"Compute standard deviation over the elements of an array"
function std(A::AbstractArray)
   A |> var |> sqrt
end

@test std(TEST_SAMPLE) ≈ Stats.std(TEST_SAMPLE)

"Compute skewness over the elements of an array"
function skewness(A::AbstractArray, mean=mean(A), var=var(A))
   n = length(A)
   (sum((A .- mean).^3) / n) / var^(3/2)
end

# TODO: Investigate why the difference is so big
@test isapprox(skewness(TEST_SAMPLE), StatsBase.skewness(TEST_SAMPLE), rtol=10e-3)

"Compute kurtosis over the elements of an array"
function kurtosis(A::AbstractArray, mean=mean(A))
   n = length(A)
   (n - 1) * n * (n + 1) / (n - 2) / (n - 3) * sum((A .- mean).^4) /
   sum((A .- mean).^2)^2 - 3(n - 1)^2 / (n - 2) / (n - 3)
end

# TODO: Investigate why the difference is so big
@test isapprox(kurtosis(TEST_SAMPLE), StatsBase.kurtosis(TEST_SAMPLE), rtol=10e-1)
```

```{julia}
import Markdown
import Plots
import StatsPlots
import Distributions as Dists

struct GeneratorTestStatisticsError
   distribution
   mean
   var
   skewness
   kurtosis
end

function GeneratorTestStatisticsError(distribution, sample)
   GeneratorTestStatisticsError(
      distribution,
      mean(sample),
      var(sample),
      skewness(sample),
      kurtosis(sample),
   )
end

function Base.show(
   io::IO,
   mime::MIME"text/markdown",
   generator_test_statistics::GeneratorTestStatisticsError
)
   distribution = generator_test_statistics.distribution
   names = ["Statistic", "mean", "variance", "skewness", "kurtosis"]
   absolute_errors = [
      "Absolute error",
      generator_test_statistics.mean - Dists.mean(distribution),
      generator_test_statistics.var - Dists.var(distribution),
      generator_test_statistics.skewness - Dists.skewness(distribution),
      generator_test_statistics.kurtosis - Dists.kurtosis(distribution),
   ]
   theoretic_values = [
      "Theoretic value",
      Dists.mean(distribution),
      Dists.var(distribution),
      Dists.skewness(distribution),
      Dists.kurtosis(distribution),
   ]
   rows = map(zip(names, absolute_errors, theoretic_values)) do (name, error, value)
      [name, error, value]
   end
   md = Markdown.MD(Markdown.Table(rows, [:l, :l, :l]))
   Base.show(io, mime, md)
end

struct GeneratorTestPlots
   cdf
   pdf
   qq
end

function Base.show(io::IO, test_plots::GeneratorTestPlots)
   p = Plots.plot(test_plots.cdf, test_plots.pdf, test_plots.qq)
   Base.display(p)
end

struct GeneratorTest
   statistics::GeneratorTestStatisticsError
   plots::GeneratorTestPlots
end

function test_generator(distribution, generator, sample_size=10^3)
   sample = [generator() for _ in 1:sample_size]
   test_statistics = GeneratorTestStatisticsError(distribution, sample)
   sample_ecdf = StatsBase.ecdf(sample)
   cdf_plot = StatsPlots.plot(sample_ecdf, label="Empirical CDF")
   Plots.plot!(
      cdf_plot, x -> Dists.cdf(distribution, x), label="Theoretic CDF"
   )
   Plots.title!(cdf_plot, "Cumulative Distribution Function")
   Plots.xlabel!(cdf_plot, "Argument")
   Plots.ylabel!(cdf_plot, "Value")

   if distribution isa Dists.DiscreteDistribution
      proportion_map = StatsBase.proportionmap(sample)
      plot_points = unique(sample)
      epmf(x) = proportion_map[x]
      pdf_plot = StatsPlots.groupedbar(
         plot_points,
         [epmf.(plot_points) (x -> Dists.pdf(distribution, x)).(plot_points)],
         labels=["Empirical PMF" "Theoretic PMF"]
      )
      Plots.title!(pdf_plot, "Probability Mass Function")
      Plots.xlabel!(pdf_plot, "Argument")
      Plots.xticks!(plot_points)
      Plots.ylabel!(pdf_plot, "Probability")
   elseif distribution isa Dists.ContinuousDistribution
      pdf_plot = Plots.histogram(sample, normalize=true, label="Histogram")
      StatsPlots.plot!(
         x -> Dists.pdf(distribution, x), label="Theoretic density"
      )
      Plots.title!(pdf_plot, "Probability density function")
      Plots.xlabel!(pdf_plot, "Argument")
      Plots.ylabel!(pdf_plot, "Probability density")
   else
      error("distribution of a mixed type is not allowed")
   end

   qq_plot = StatsPlots.qqplot(distribution, sample)
   Plots.title!(qq_plot, "Quantile-Quantile")
   Plots.xlabel!(qq_plot, "Theoretic quantile")
   Plots.ylabel!(qq_plot, "Empirical quantile")

   test_plots = GeneratorTestPlots(cdf_plot, pdf_plot, qq_plot)
   GeneratorTest(test_statistics, test_plots)
end
```

# Inverse Transform Sampling

Inverse Transform Sampling (ITS) is one of the most fundamental techniques
for generating samples from a specified probability distribution based on
samples from a uniform distribution. <br> The cumulative distribution
function of a given probability distribution is a function $F: R
\rightarrow R$. The cumulative distribution function uniquely defines the
probability distribution. If the inverse function to the cumulative
distribution function (CDF), denoted as $F^{-1}(x)$, exists, and a random
variable $U$ follows the distribution $F(x)$, then the random variable $X$
defined as $X = F^{-1}(U)$ follows a distribution with the CDF $F(x)$. In
this method, the generalized quantile is also sufficient. Inversion cannot
be used in the discrete case. In that case, the inverse function of the
cumulative distribution does not exist. We have: $$\mathbb P (X \leq x) =
\mathbb P(F^{-1}(U) \leq x) = \mathbb P (U \leq F(x)) = F(x).$$  A sequence
of pseudorandom numbers is generated: $U_1, U_2, \dots, U_n \in (0,1)$,
which is then transformed into a sequence $X_1, X_2, \dots, X_n \in
(-\infty, \infty)$. The numbers $X_i$ have a probability distribution with
the cumulative distribution function F. The inversion of the cumulative
distribution function can also be used in the case of discrete probability
distributions with probabilities $p_k = \mathbb P (X = k)$, where $k =
0,1,2, \dots$, which can be generated using the sequence $U_i$ according to
the formula:  $$X_n = min \{ k: U_n \leq \sum\limits_{i=0}^k p_i\}, \quad n
= 1, 2, 3, \dots$$ Below code demonstrates an example of using the Inverse
Transform Sampling (ITS) method to generate samples from a continuous
distribution, specifically the Weibull $\mathcal{We}(2,1)$ distribution
with shape parameter 2 and scale parameter 1. The generator takes a
function as an argument, which in this case is an anonymous function that
applies the inverse transformation formula for the Weibull distribution:
$\sqrt{-\log(1 - x)}$. This function maps values from a uniform
distribution (generated internally) to values from the Weibull
distribution.  Since the uniform distribution has a constant density of 1
on the interval (0,1), the ratio simplifies to $\frac{p(x)}{q(x)} =
p(x)$. To find the maximum value of $p(x)$, we can examine the derivative:
$\frac{dp(x)}{dx} = 2(1-x^2)e^{-x^2} - 4x^2 e^{-x^2}$. Setting the
derivative equal to zero, simplifying further and solving for $x^2$, we
get: $x^2 = \frac 13$, so $x = \sqrt{\frac 13}$. Therefore, the maximum
value of $p(x)$ occurs at $x = \sqrt{\frac{1}{3}}$. Substituting this value
back into the density function, we can calculate the maximum ratio:
$\frac{p(x)}{q(x)} = p(\sqrt{\frac 13}) = 2 \sqrt{\frac 13 e^{(-\frac
13)^2}} =  2 \sqrt{\frac 13 e^{- \frac 13}}$. Hence, the constant $c$ for
the Weibull $(2,1)$ distribution with Uniform $(0,1)$ is
$2\sqrt{\frac{1}{3}}e^{-\frac{1}{3}}$.<br> The following plots represent
the following: the cumulative distribution function, where the Empirical
CDF is depicted in blue and the Theoretical CDF in red. It can be observed
that both curves largely overlap, indicating a correctly implemented
algorithm. The next plot is the Probability Density Function, which shows a
histogram and the theoretical density. The last of the plots is the
QQ-plot, also known as the Quantile-Quantile plot. The empirical quantiles
are plotted on the Y-axis, while the theoretical quantiles are described on
the X-axis. Below, in the table, basic descriptive statistics such as mean,
variance, skewness, and kurtosis are presented for both absolute error and
theoretical error. <br> Next code demonstrates an example of using the
inverse transform sampling method to generate samples from a discrete
distribution, specifically the Bernoulli $\mathcal B(\frac12)$ distribution
with a success probability of $\frac12$. This distribution represents a
binary random variable that takes either the value 0 or 1. If the input
value x is less than or equal to 1/2, the function returns 0; otherwise, it
returns 1. This function serves as the inverse of the cumulative
distribution function for the Bernoulli distribution. This generator
applies the inverse transformation defined by bernoulli_quantile to map
values from a uniform distribution to values from the Bernoulli
distribution. Just as with the presentation of results for the continuous
probability distribution, the results for the discrete probability
distribution version are also presented in the form of three plots. Just
like in the previous calculation, the Bernoulli(1/2) distribution has a
constant $c = \frac 12$.<br> The Cumulative Distribution Function plot
shows that the empirical CDF and the theoretical CDF are very close to each
other. The only significant difference is that the theoretical CDF starts
from the point (0, 0.5), while the empirical CDF starts from the initial
point (0, 0). As the curves progress, we observe slight differences. The
next plot represents the Probability Mass Function, comparing the empirical
PMF with the theoretical PMF. It can be noticed that the probability values
are close to each other, with differences not exceeding 0.1. The last plot
is the Quantile-Quantile Plot, which exhibits a linear character. It
behaves almost like the line y=x. Below the plots, a table is presented,
showing basic descriptive statistics such as mean, variance, skewness, and
kurtosis. It can be observed that the absolute error for the theoretical
value in each row of the table is significantly small, indicating an
acceptable level of error.

```{julia}
function InverseTransformSamplingGenerator(quantile)
   () -> begin
      u = rand()
      quantile(u)
   end
end
```

## Continuous Example (Weibull(2, 1))
```{julia}
distribution = Dists.Weibull(2)
generator = InverseTransformSamplingGenerator(x -> (-log(1 - x))^(1/2))
test = test_generator(distribution, generator)
display(test.plots); display(test.statistics)
```

## Discrete Example (Bernoulli(1/2))
```{julia}
distribution = Dists.Bernoulli(1/2)
bernoulli_quantile(x) = x <= 1/2 ? 0 : 1
generator = InverseTransformSamplingGenerator(bernoulli_quantile)
test = test_generator(distribution, generator)
display(test.plots); display(test.statistics)
```

# Rejection sampling

First, consider the case for a continuous probability distribution: Let $X$
be a random variable with density $f$. The function $F$ is continuous and
only takes positive values on $[a,b]$ except for 0. Since continuity holds,
we can determine the maximum. Executing the algorithm involves generating a
random variable $X$ with density $f$. A random variable $Y$ is efficiently
generated with density $g$. Importantly, $Y$ should take values from the
same set as $X$. The constant $c$ is determined such that: $$\sup\limits_{x
\in \mathbb R} \frac{f(x)}{g(x)} \leq c \leq \infty.$$ The constant $c$ is
optimally determined when the supremum is equal to a constant $c$. In
rejection sampling, the constant $c$ can be described as the acceptance
constant. The choice of the constant $c$ is crucial in achieving an optimal
value between the efficiency of the sampling algorithm and the accuracy of
the generated samples. If $U \leq \frac{f(Y)}{c \cdot g(Y)},$then $X$ is
returned as $Y$. If not, another $Y$ will be generated again. <br> In the
example below, the distribution Semicircle$(1)$ with Uniform$(-1,1)$ is
presented, and two probability density functions (PDFs), semicircle_pdf and
uniform_pdf, are defined. When the range of $x$ extends beyond $[-1,1]$,
the PDF value is set to 0. The generator is tested by calling the
test_generator function. To find the constant $c$ for the Semicircle$(1)$
distribution with Uniform$(-1,1)$, we need to determine the value of $c$
that satisfies the condition: $\sup_{x \in \mathbb R} \frac{f(x)}{g(x)}
\leq c \leq \infty$, The probability density function (PDF) for the
Semicircle(1) distribution is given by: $f(x) = (\frac 2pi) *\ sqrt(1 -
x^2)$  for $-1 \leq  x \leq 1$, and 0 elsewhere. The PDF for the
Uniform$(-1,1)$ distribution is constant and given by: $g(x) = \frac 12$
for $-1 \leq x \leq 1$, and 0 elsewhere. To find the maximum ratio, we
evaluate it at the critical points of the Semicircle(1) distribution, which
are $x = -1$ and $x = 1$: $\frac{f(-1)}{g(-1)} = (\frac 2/pi) * \sqrt(1 -
(-1)^2)) / (\frac 12) = (\frac 2 \pi) * \sqrt 1 = \frac 2\pi, \
\frac{f(1)}{g(1)}= ((\frac 2 \pi) *\frac{ \sqrt(1 - 1^2)) }{(\frac 12)} =
0. Therefore, the supremum of the ratio is \frac 2 \pi.

Hence, the constant $c$ for the Semicircle(1) distribution with
Uniform(-1,1) is\frac 2\pi. <br> The case for a discrete probability
distribution: Let CDF, $F$ for $X$: $$F(x) = \sum\limits_{i \leq j}
p(x_i).$$ Discrete random variables can be generated by slicing up the
interval $(0, 1)$ into subintervals which define a partition of $(0, 1)$:
$$(0, F(x_1)),(F(x_2), F(x_3)), \dots, (F(x_{k-1}), 1).$$ Generating random
variables U from a uniform distribution on the interval (0, 1) and
determining the subinterval into which U falls. A realization of a discrete
random variable X is generated according to the given formula: $$\mathbb{P}
(X = i) = p_i, \ i = 1,2,\dots, n.$$ It is possible to generate another
random variable Y with a distribution defined by the formula: $$\mathbb{P}
(Y = i) = q_i, \ i = 1,2, \dots, n.$$ Then, a random variable $U$ is
generated from the uniform distribution $\mathcal U(0,1)$, such that $U$ is
independent of $Y$. If $U$ is evaluated to satisfy the condition $U \leq
\frac{pY}{c \cdot q_y}$, where $c = \max\limits_i \frac{p_i}{q_i}$, then
$X$ is set to be equal to $Y$. Otherwise, we go back to the beginning of
the algorithm and generate $Y$ again. <br>

In the example below, rejection sampling is used for a discrete probability
distribution. The Bernoulli distribution $\mathcal{Be}(\frac 14)$ with
Uniform(0,1) is chosen. In the code, two probability mass functions (PMF)
are defined: bernoulli_pmf and uniform_pmf. The bernoulli_pmf(x) function
returns the PMF value based on the input x: $\frac 34$, $\frac 14$, and
zero for $x=0$, $x=1$, and any other value respectively. Similarly, the
uniform_pmf(x) function returns the PMF value based on the input x: $\frac
12$ for $x=0$ or $x=1$, and zero for any other value. Then, the generator
is tested by calling the test_generator function. To find the maximum
ratio, we compare the ratios $\frac{p_i}{q_i}$ for each outcome:
$\frac{\frac{3}{4}}{\frac{1}{2}} =
\frac{3}{2}$,$\frac{\frac{1}{4}}{\frac{1}{2}} = \frac{1}{2}$The maximum
ratio is $\frac{3}{2}$. Therefore, the constant $c$ for the Bernoulli
distribution with parameter $\frac{1}{4}$ and Uniform(0,1) is
$\frac{3}{2}$.<br>

Analyzing the generated graphs below, one can assess how well the random
data generator reflects the expected distributions. The closer the
empirical graphs are to the theoretical ones, the more accurate and
reliable the generator is.

Cumulative Distribution Function (CDF) graph shows the comparison between
the empirical cumulative distribution function, calculated based on the
generated sample, and the theoretical cumulative distribution function for
the given distribution. The closer the empirical curve aligns with the
theoretical curve, the better the generator reflects the expected
distribution.

Probability Density Function (PDF) graph  presents a comparison between the
normalized histogram of the generated sample and the theoretical
probability density function for the given distribution. The closer the
histogram resembles the theoretical density function, the better the
generator reflects the probability distribution.

Quantile-Quantile (QQ) graph compares the empirical quantiles, calculated
based on the generated sample, with the theoretical quantiles for the given
distribution. If the points on the graph lie along a straight line, it
indicates that the generated data follows a distribution similar to the
theoretical one.

Probability Mass Function (PMF) graph illustrates the comparison between
two types of bars: empirical PMF and theoretical PMF. The height of each
blue bar corresponds to the estimated probability of a particular outcome
occurring in the sample, while the red bar represents the theoretical PMF
values for the given probability distribution. We observe slight
differences in the heights of the blue and red bars, indicating that the
function has been constructed correctly, meaning that the designed
generator reflects the probability distribution accurately.

```{julia}
function RejectionSamplingGenerator(pdf, aux_pdf, aux_gen, multiplier)
   () -> begin
      while true
         x = aux_gen()
         if (u = rand()) <= pdf(x) / (multiplier * aux_pdf(x))
            return x
         end
      end
   end
end
```

## Continuous Example (Semicircle(1) with Uniform(-1, 1))

```{julia}
distribution = Dists.Semicircle(1)
semicircle_pdf(x) = -1 <= x <= 1 ? 2/π * sqrt(1 - x^2) : 0
uniform_pdf(x) = -1 <= x <= 1 ? 1 : 0
generator = RejectionSamplingGenerator(
   semicircle_pdf, uniform_pdf, () -> 2 * rand() - 1, 1
)
test = test_generator(distribution, generator)
display(test.plots); display(test.statistics)
```

## Discrete Example (Bernoulli(1/4) with Uniform(0..1))

```{julia}
distribution = Dists.Bernoulli(1/4)

bernoulli_pmf(x) = begin
   if x == 0
      3/4
   elseif x == 1
      1/4
   else
      0
   end
end

uniform_pmf(x) = begin
   if x == 0
      1/2
   elseif x == 1
      1/2
   else
      0
   end
end

generator = RejectionSamplingGenerator(
   bernoulli_pmf, uniform_pmf, () -> rand([0, 1]), 3/2
)
test = test_generator(distribution, generator)
display(test.plots); display(test.statistics)
```

# Normal distribution generation methods

In this section of the report, we will explore different methods for
generating random numbers from a normal distribution. The normal
distribution is a widely used probability distribution in various fields -
for example statistical analysis. Generating random (pseudo-random) numbers
from a normal distribution is essential in many computer simulations. We
will focus on three commonly employed methods for generating random numbers
from a normal distribution: the Box-Muller transform, the Polar method and
rejection sampling. The last subsection is benchmark.

## Box-Muller transform

The Box-Muller transform is a classic method that transforms uniformly
distributed random numbers into independent and identically distributed
$(i.i.d)$ standard normal random numbers. It is based on the trigonometric
functions. <br> The algorithm is as follows: We generate two random
variables, $U_1 \sim \mathcal U (0,1)$ and $U_2 \sim \mathcal U(0,1)$, such
that $U_1$ and $U_2$ are independent. Then, we define $R = \sqrt{-2
\log(U_1)}$ and $\Phi = 2\pi U_2$. Next, the random variables $X$ and $Y$
are represented in polar form as $X = R \cos(\Phi)$ and $Y = R
\sin(\Phi)$. Therefore, they can be expressed as $X = \sqrt{-2 \log(U_1)}
\cos (2\pi U_2) $ and $Y = \sqrt{-2 \log(U_1)} \sin (2\pi U_2)$. This
implies that $U_1 = e^{-(X^2 + Y^2)/2}$ and $U_2 = \frac{1}{2\pi} \arctan
(\frac XY)$. By calculating the Jacobian matrix equal, we obtain the joint
probability density function as follows: $$f_{X,Y}(x,y) =
\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \cdot \frac{1}{\sqrt{2\pi}}
e^{-\frac{y^2}{2}}.$$ Recall that $$f_{X,Y} (x,y) = f_{X}(x) \cdot f_Y (y)
\Leftrightarrow X \perp Y.$$  Therefore, we conclude that $X \perp Y$, with
$X$ following a standard normal distribution $\mathcal N (0,1)$ and $Y$
following a standard normal distribution $\mathcal N (0,1)$. <br> The
function takes two optional arguments, mu and sigma, representing the mean
and standard deviation of the desired normal distribution,
respectively. The default values in function are $mu=0$ and
$sigma=1$. Next, the Box-Muller transform is applied to generate two
independent standard normal random variables The transformation involves
taking $\sqrt{-2 \cdot \log(u)} \cdot$ and multiplying it by $\cos(2\pi v$)
and $\sin(2 \pi v)$ respectively. These calculations generate two standard
normal random variables. <br>

```{julia}
function BoxMullerGenerator(μ=0, σ²=1)
   σ = sqrt(σ²)
   () -> begin
      u₁, u₂ = rand(2)
      z₁, z₂ = sqrt(-2 * log(u₁)) * cos(2π * u₂), sqrt(-2 * log(u₁)) *
         sin(2π * u₂)
      [σ * z₁ + μ, σ * z₂ + μ]
   end
end
```

## Polar method

The Polar method generates pairs of normally distributed random numbers
using the transformation of uniform random variables. It employs the
rejection technique to ensure efficiency and accuracy in generating random
numbers. <br> PolarMethod function is generating random variables from a
normal distribution. It takes two arguments: the mean and the variance. In
each iteration, two independent random variables are generated from a
uniform distribution in the range $[-1, 1]$. The squared distance $R^2$,
from the origin to the point defined by these two generated random
variables is calculated. The algorithm generates new samples as long as the
condition $R^2 \leq 1$ is satisfied. As a result, the function generates
pairs of random variables following a normal distribution with parameters
$\mu$ and $\sigma^2$.

```{julia}
function PolarMethod(μ=0, σ²=1)
   σ = sqrt(σ²)
   () -> begin
      while true
         u₁, u₂ = 2 .* rand(2) .- 1
         R² = u₁^2 + u₂^2
         if R² <= 1
            z₁, z₂ = sqrt(-2 * log(R²) / R²) * u₁, sqrt(-2 * log(R²) / R²) * u₂
            return [σ * z₁ + μ, σ * z₂ + μ]
         end
      end
   end
end
```

## Rejection sampling

Rejection sampling is a versatile method for generating random numbers from
various distributions, including the normal distribution. This method was
described in the previous section with two examples: a continuous example
using the Semicircle$(1)$ distribution with Uniform$(-1,1)$, and a discrete
example using the Bernoulli$(\frac 14)$ distribution with
Uniform$(0,1)$. In this section, we will focus on the application of
rejection sampling for generating random numbers from a normal
distribution, which is a continuous probability distribution. <br> The
difference between this example and the one generating from the
Semicircle$(1)$ with Uniform$(-1,1)$ is the auxiliary distribution used for
generating random variables. In the case of rejection sampling for the
normal distribution, the auxiliary distribution is the exponential
distribution. Therefore, the ratio of the maximum value of the probability
density function to the auxiliary probability density function is
different.

```{julia}
normal_pdf(x) = 1/2π * exp(-x^2/2)
laplace_pdf(x) = 1/2 * exp(-abs(x))
exponential_quantile(x) = -log(1 - x)
laplace_generator = ComposedFunction(
   x -> x * rand([-1, 1]),
   InverseTransformSamplingGenerator(exponential_quantile),
)
normal_rejection_sampling_generator = RejectionSamplingGenerator(
   normal_pdf,
   laplace_pdf,
   laplace_generator,
   20
)
```

## Benchmarks

The benchmark aims to compare the performance of three methods for
generating random variables: Box-Muller transform, Polar method, and
rejection sampling. Performance tests are conducted for each method and
sample size, and the results are stored in an array. The median execution
times for each method and sample size have been computed. In the summary,
plots are generated to depict the performance of the methods based on the
sample size. <br> <br> Analyzing the graphs presented in the benchmark, it
can be observed that for each of the four sample sizes, the rejection
sampling method has the longest execution time. The differences between the
polar method and the Box-Muller method are small and do not increase with
the increase in sample size. With these two methods, the execution time
slightly increases with the increase in the sample size. However, the
execution time of rejection sampling increases proportionally with the
increase in the sample size.

```{julia}
import BenchmarkTools as BT
import DataFrames as DF
using LaTeXStrings

exponents = 1:4
sample_sizes = [10^i for i in exponents]
generators = [
   BoxMullerGenerator(), PolarMethod(), normal_rejection_sampling_generator
]
results = Array{Any}(undef, (length(generators), length(sample_sizes)))

results = map(Base.Iterators.product(exponents, 1:3)) do (i, j)
   result = BT.@benchmark [$generators[$j]() for _ in 1:$sample_sizes[$i]]
   result.times
end

median_results = map(Stats.median, results)

the_plots = map(exponents) do exponent
   p = StatsPlots.groupedbar(
      median_results[exponent, :],
      group=["Box-Muller", "Polar", "Rejection"],
      legend=:none,
   )
   Plots.title!(p, L"Sample size $10^%$exponent$")
   Plots.ylabel!(p, "Time")
   Plots.xticks!(p, [0])
end
legend = Plots.plot(
   [0 0 0],
   showaxis = false,
   grid = false,
   label=["Box-Muller" "Polar" "Rejection"]
)
p = Plots.plot(
      the_plots...,
      legend,
      legend_title="Median",
      layout=Plots.@layout [Plots.grid(2, 2) l{0.2w}]
   )
```

# The Ziggurat Method

To perform the Ziggurat method, it was necessary to import the find_zero
function from the Roots library. The ParetoGenerator function is
responsible for generating a Pareto distribution using the Ziggurat
method. This function takes parameters that describe the shape and scale of
the distribution. Next, the structure of the Ziggurat for the generator is
created, which can be seen as points that assist in generating the Pareto
distribution. The values of these points are calculated. To successfully
carry out the algorithm, the probability density function, the inverse
function of the density function, the tail of the distribution, and the
quantile of the distribution are utilized.

The Ziggurat algorithm is  an efficient alternative to other algorithms
like the Box-Muller transform. The key idea is to generate random numbers
by selecting a strip and a position within that strip. First, we need to
calculate the heights and areas of each strip in the ziggurat structure
based on the PDF of the target distribution. Next step is  selecting  a
random strip and a position within that strip. The position is chosen
uniformly within the strip's boundaries. Accept or reject  step about
compare the generated position with the corresponding PDF value. If the
position is below the PDF value, accept it as a random number from the
distribution. Otherwise, reject it and repeat the process. After the
algorithm is executed, a test of the generator from the Pareto distribution
with parameters 5 and 1 is performed. Samples from the actual distribution
and samples from the implemented generator are generated. Finally, three
comparative plots are presented. The first plot shows the empirical
cumulative distribution function (CDF) compared to the theoretical CDF. It
can be observed that the closer the curves are to each other, the better
the generator replicates the distribution. The second plot displays the
probability density function (PDF), which represents the normalized
histogram of the generated sample along with the theoretical PDF of the
Pareto distribution. The histogram accurately reflects the theoretical
density values. The last plot is the QQ-plot, which compares the empirical
quantiles calculated based on the generated sample. The generated data
follows a distribution similar to the theoretical Pareto distribution.

## Peculiarities

There is some peculiarity with using this method with distributions having
the support being an interval not beginning in $0$. The original algorithm
samples points from the uniform distribution over subintervals of the
support, the subintervals having the left ends equal to $0$, only differing
in right ends, corresponding to strips' right end coordinates. The authors
see two solutions.

One can try to translate the functions defining the distribution, so it is
properly situated. The other comes down to altering the uniform sampling
procedure, so the points are properly sampled from shifted intervals. The
latter comes with the additional cost of addition and subtraction with each
uniform sampling, but the former requires these only in the initial
procedure of identifying the ziggurat's structure and one addition on
returning the final realization. But this also requires one to transform
the PDF and its inverse symbolically beforehand, so the user cannot blindly
pass well-known functions to the general version of the algorithm. But in
the other solution, one would also have to give a specification of uniform
distributions to sample from (the shift parameter), so the authors settled
down on the idea of transforming density, which is equivalent to composing
it with a translation by a constant factor and finding the inverse.

There is also one subtlety with a method showed in the article. During
determination of the strips' coordinates by zeroing the $z$ function, for
example by using bisection method, one has to evaluate it possibly at
negative values, where it can be undefined. It happened in the case of the
Pareto distribution in the following example. One can avoid it evaluating
the $z$ function in the absolute values of points instead. The results may
not be formally correct, but they are good in our example, so the authors
settle on this solution.

## Example (Pareto distribution)

It is easy to determine the area $A_{\text {tail}}$ of the tail, as it is
given by the definition of the Pareto distribution. That is
$$
A_{\text {tail}} = \bar F = \begin {cases}
   {\left(\frac {x_m} \alpha\right)}^\alpha, x_m > x,
   1, x_m \leq x,
\end {cases}
$$
where $x_m$ is a scale parameter and $\alpha$ is a shape parameter.

In this case, the distribution's support is $[x_m, \infty]$. Therefore, in
the beginning, the distribution specific functions are shifted by $x_m$
parameter, as mentioned in @sec:Peculiarities.

### Implementation

```{julia}
import Roots: find_zero

"""
Return a Pareto distribution generator

The returned generator uses the ziggurat method.
"""
function ParetoGenerator(shape, scale)
   n_strips = 256
   # Pareto specific functions
   # Usual Pareto's PDF composed with translation
   pdf(x) = (shape * scale^shape) / (x + scale)^(shape + 1)
   inverted_pdf(y) = (shape * scale^shape / y)^(1/(shape + 1)) - scale
   tail(x) = x >= 0 ? (scale / (x + scale))^shape : 1
   # Used for inverse sampling from the tail
   quantile(x) = scale * (1 - x)^(-1/shape) - scale

   # Determine the ziggurat's structure
   step_points = Array{Float64}(undef, n_strips)
   function to_be_zeroed(rightmost)
      area = rightmost * pdf(rightmost) + tail(rightmost)
      step_points[end] = rightmost
      for i in lastindex(step_points)-1 : -1 : firstindex(step_points)+1
         previous = step_points[i+1]
         # `abs` prevents invalid values in `inverted_pdf`
         height = area / abs(previous) + pdf(previous)
         step_points[i] = inverted_pdf(height)
      end
      first = step_points[begin+1]
      return area - first * (pdf(0) - pdf(first))
   end

   # Initial guess
   somewhere_far = quantile((n_strips-1)/n_strips)
   rightmost = find_zero(to_be_zeroed, somewhere_far)
   area = rightmost * pdf(rightmost) + tail(rightmost)

   for i ∈ lastindex(step_points)-1 : -1 : firstindex(step_points)+1
      prev = step_points[i+1]
      step_points[i] = inverted_pdf(area / prev + pdf(prev))
   end
   # The special value
   step_points[begin] = pdf(rightmost) * rightmost / area
   
   () -> begin
      while true
         # Choose a strip
         i = rand(UInt8) + 1
         x = rand() * step_points[i]
         if i != 1 && x <= step_points[i-1]
            return x + scale
         end
         # Choose from the tail
         if i == 1
            x = rand() * rightmost
            if x <= step_points[end]
               return x + scale
            end
         end
         if rand() * (
            pdf(step_points[i-1]) - pdf(step_points[i])) <
               pdf(x) - pdf(step_points[i]
         )
            return x + scale
         end
      end
   end
end
```
```{julia}
distribution = Dists.Pareto(5, 1)
generator = ParetoGenerator(5, 1)
test = test_generator(distribution, generator)

# Make plots more readable, correcting for Pareto's infinite tail
Plots.xlims!(test.plots.cdf, 0.9, 2.5)
Plots.xlims!(test.plots.pdf, 0.9, 2.5)
Plots.xlims!(test.plots.qq, 0.9, 2.5)
Plots.ylims!(test.plots.qq, 0.9, 2.5)

display(test.plots); display(test.statistics)
```
