{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c9adcb-e1bc-4812-af13-9998ec9d9dcd",
   "metadata": {},
   "source": [
    "# Rejection Sampling\n",
    "\n",
    "First, consider the case for a continuous probability distribution:\n",
    "Let $X$ be a random variable with density $f$. The function $F$ is continuous and only takes positive values on $[a,b]$ except for 0. Since continuity holds, we can determine the maximum.\n",
    "Executing the algorithm involves generating a random variable $X$ with density $f$. A random variable $Y$ is efficiently generated with density $g$. Importantly, $Y$ should take values from the same set as $X$. The constant $c$ is determined such that:\n",
    "$$\\sup\\limits_{x \\in \\mathbb R} \\frac{f(x)}{g(x)} \\leq c \\leq \\infty.$$ The constant $c$ is optimally determined when the supremum is equal to a constant $c$. In rejection sampling, the constant $c$ can be described as the acceptance constant. The choice of the constant $c$ is crucial in achieving an optimal value between the efficiency of the sampling algorithm and the accuracy of the generated samples. If $U \\leq \\frac{f(Y)}{c \\cdot g(Y)},$then $X$ is returned as $Y$. If not, another $Y$ will be generated again. <br>\n",
    "In the example below, the distribution Semicircle$(1)$ with Uniform$(-1,1)$ is presented, and two probability density functions (PDFs), semicircle_pdf and uniform_pdf, are defined. When the range of $x$ extends beyond $[-1,1]$, the PDF value is set to 0. The generator is tested by calling the test_generator function. <br>\n",
    "The case for a discrete probability distribution:\n",
    "Let CDF, $F$ for $X$: $$F(x) = \\sum\\limits_{i \\leq j} p(x_i).$$\n",
    "Discrete random variables can be generated by slicing up the interval $(0, 1)$ into subintervals which define a partition of $(0, 1)$: $$(0, F(x_1)),(F(x_2), F(x_3)), \\dots, (F(x_{k-1}), 1).$$ Generating random variables U from a uniform distribution on the interval (0, 1) and determining the subinterval into which U falls. \n",
    "A realization of a discrete random variable X is generated according to the given formula: $$\\mathbb{P} (X = i) = p_i, \\ i = 1,2,\\dots, n.$$ It is possible to generate another random variable Y with a distribution defined by the formula: $$\\mathbb{P} (Y = i) = q_i, \\ i = 1,2, \\dots, n.$$ Then, a random variable $U$ is generated from the uniform distribution $\\mathcal U(0,1)$, such that $U$ is independent of $Y$. If $U$ is evaluated to satisfy the condition $U \\leq \\frac{pY}{c \\cdot q_y}$, where $c = \\max\\limits_i \\frac{p_i}{q_i}$, then $X$ is set to be equal to $Y$. Otherwise, we go back to the beginning of the algorithm and generate $Y$ again. <br>\n",
    "\n",
    "In the example below, rejection sampling is used for a discrete probability distribution. The Bernoulli distribution $\\mathcal{Be}(\\frac 14)$ with Uniform(0,1) is chosen. In the code, two probability mass functions (PMF) are defined: bernoulli_pmf and uniform_pmf. The bernoulli_pmf(x) function returns the PMF value based on the input x: $\\frac 34$, $\\frac 14$, and zero for $x=0$, $x=1$, and any other value respectively. Similarly, the uniform_pmf(x) function returns the PMF value based on the input x: $\\frac 12$ for $x=0$ or $x=1$, and zero for any other value. Then, the generator is tested by calling the test_generator function. <br>\n",
    "\n",
    "\n",
    "Analyzing the generated graphs below, one can assess how well the random data generator reflects the expected distributions. The closer the empirical graphs are to the theoretical ones, the more accurate and reliable the generator is.\n",
    "\n",
    "Cumulative Distribution Function (CDF) graph shows the comparison between the empirical cumulative distribution function, calculated based on the generated sample, and the theoretical cumulative distribution function for the given distribution. The closer the empirical curve aligns with the theoretical curve, the better the generator reflects the expected distribution.\n",
    "\n",
    "Probability Density Function (PDF) graph  presents a comparison between the normalized histogram of the generated sample and the theoretical probability density function for the given distribution. The closer the histogram resembles the theoretical density function, the better the generator reflects the probability distribution.\n",
    "\n",
    "Quantile-Quantile (QQ) graph compares the empirical quantiles, calculated based on the generated sample, with the theoretical quantiles for the given distribution. If the points on the graph lie along a straight line, it indicates that the generated data follows a distribution similar to the theoretical one.\n",
    "\n",
    "Probability Mass Function (PMF) graph illustrates the comparison between two types of bars: empirical PMF and theoretical PMF. The height of each blue bar corresponds to the estimated probability of a particular outcome occurring in the sample, while the red bar represents the theoretical PMF values for the given probability distribution. We observe slight differences in the heights of the blue and red bars, indicating that the function has been constructed correctly, meaning that the designed generator reflects the probability distribution accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107fa49c-e41c-4fd7-9e6f-416a8e8b26a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
